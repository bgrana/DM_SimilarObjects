{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir = \"./docs\"\n",
    "onlyfiles = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "docs = []\n",
    "for fname in onlyfiles:\n",
    "    with open(join(input_dir, fname), \"r\") as file:\n",
    "        docs += [file.read()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean documents (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [re.sub('\\W+', ' ', doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shingles' hash values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Shingling():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.hash = hash # Use python built-in hashing function\n",
    "    def transform(self, doc):\n",
    "        # Compute shingles\n",
    "        shingles = np.array([doc[i:i+self.k] for i in range(0, len(doc) - self.k + 1)])\n",
    "        # Filter out duplicates and sort\n",
    "        hashes = sorted(set([self.hash(shingle) for shingle in shingles]))\n",
    "        \n",
    "        return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh = Shingling(9)\n",
    "sets = [sh.transform(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare shingle hashes with Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_shingles(a, b):\n",
    "    \"\"\"Returns the jaccard similarity between two sets.\n",
    "        a -> set\n",
    "        b -> set\n",
    "    \"\"\"\n",
    "    return len(set(a) & set(b)) / len(set(a) | set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('B1.txt', 'B2.txt'), 1.0),\n",
       " (('A1.txt', 'A1small.txt'), 0.9586281981491562),\n",
       " (('B1.txt', 'B1small.txt'), 0.45179335307666996),\n",
       " (('B2.txt', 'B1small.txt'), 0.45179335307666996)]"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = {(onlyfiles[i],onlyfiles[j]): compare_shingles(sets[i], sets[j]) \n",
    "                for i in range(0, len(docs)) \n",
    "                for j in range(i+1, len(docs))}\n",
    "# Show similarities greater than a threshold\n",
    "sorted([(k,v) for k,v in similarities.items() if v > 0.4], key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute minHash signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MinHashing():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.c = 4294967311 # Big prime\n",
    "        self.a_coeffs = random.sample(range(1, 2**31), k)\n",
    "        self.b_coeffs = random.sample(range(1, 2**31), k)\n",
    "        \n",
    "    def _hash(self, x, i):\n",
    "        return (self.a_coeffs[i]*x + self.b_coeffs[i])%self.c\n",
    "        \n",
    "    def transform(self, shingles):\n",
    "        signatures = np.array([[self._hash(shingle, i) for shingle in shingles] for i in range(self.k)])\n",
    "        return np.array([l[np.argmin(l)] for l in signatures])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mh = MinHashing(100)\n",
    "signatures = [mh.transform(s) for s in sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare minHash signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_signatures(a, b):\n",
    "    \"\"\"Computes the similarity between 2 signatures.\n",
    "        a: numpy array\n",
    "        b: numpy array\n",
    "    \"\"\"\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Signatures lengths differ.\")\n",
    "    return np.mean(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('B1.txt', 'B2.txt'), 1.0),\n",
       " (('A1.txt', 'A1small.txt'), 0.96999999999999997),\n",
       " (('B1.txt', 'B1small.txt'), 0.56000000000000005),\n",
       " (('B2.txt', 'B1small.txt'), 0.56000000000000005)]"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimations = {(onlyfiles[i],onlyfiles[j]):compare_signatures(signatures[i], signatures[j]) \n",
    "                for i in range(0, len(signatures)) \n",
    "                for j in range(i+1, len(signatures))}\n",
    "# Show similarity estimations greater than a threshold\n",
    "sorted([(k,v) for k,v in estimations.items() if v > 0.4], key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare minHash estimations against real similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('B1.txt', 'B1small.txt'), 0.1082066469233301),\n",
       " (('B2.txt', 'B1small.txt'), 0.1082066469233301),\n",
       " (('A1small.txt', 'a0200029.txt'), 0.069539628563955),\n",
       " (('A1.txt', 'a0200029.txt'), 0.054129032258064508),\n",
       " (('B1.txt', 'a0200021.txt'), 0.053495934959349588),\n",
       " (('B2.txt', 'a0200021.txt'), 0.053495934959349588)]"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = {(onlyfiles[i],onlyfiles[j]):abs(estimations[(onlyfiles[i],onlyfiles[j])] - similarities[(onlyfiles[i],onlyfiles[j])])\n",
    "           for i in range(0, len(docs)) \n",
    "           for j in range(i+1, len(docs))}\n",
    "# Show errors greater than a threshold (e.g. 5%)\n",
    "sorted([(k,v) for k,v in errors.items() if v > 0.05], key=itemgetter(1), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
