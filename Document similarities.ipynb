{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt,log\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.special import lambertw\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir = \"./docs\"\n",
    "onlyfiles = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "docs = []\n",
    "for fname in onlyfiles:\n",
    "    with open(join(input_dir, fname), \"r\") as file:\n",
    "        docs += [file.read()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean documents (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [re.sub('\\W+', ' ', doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shingles' hash values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Shingling():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.hash = hash # Use python built-in hashing function\n",
    "    def transform(self, doc):\n",
    "        # Compute shingles\n",
    "        shingles = np.array([doc[i:i+self.k] for i in range(0, len(doc) - self.k + 1)])\n",
    "        # Filter out duplicates and sort\n",
    "        hashes = sorted(set([self.hash(shingle) for shingle in shingles]))\n",
    "        \n",
    "        return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh = Shingling(9)\n",
    "sets = [sh.transform(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare shingle hashes with Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_shingles(a, b):\n",
    "    \"\"\"Returns the jaccard similarity between two sets.\n",
    "        a -> set\n",
    "        b -> set\n",
    "    \"\"\"\n",
    "    return len(set(a) & set(b)) / len(set(a) | set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('B2.txt', 'B1.txt'), 1.0),\n",
       " (('A1small.txt', 'A1.txt'), 0.9586281981491562),\n",
       " (('B1small.txt', 'B1.txt'), 0.45179335307666996),\n",
       " (('B1small.txt', 'B2.txt'), 0.45179335307666996)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = {(onlyfiles[i],onlyfiles[j]): compare_shingles(sets[i], sets[j]) \n",
    "                for i in range(0, len(docs)) \n",
    "                for j in range(i+1, len(docs))}\n",
    "# Show similarities greater than a threshold\n",
    "sorted([(k,v) for k,v in similarities.items() if v > 0.4], key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute minHash signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MinHashing():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.c = 4294967311 # Big prime\n",
    "        self.a_coeffs = random.sample(range(1, 2**31), k)\n",
    "        self.b_coeffs = random.sample(range(1, 2**31), k)\n",
    "        \n",
    "    def _hash(self, x, i):\n",
    "        return (self.a_coeffs[i]*x + self.b_coeffs[i])%self.c\n",
    "        \n",
    "    def transform(self, shingles):\n",
    "        signatures = np.array([[self._hash(shingle, i) for shingle in shingles] for i in range(self.k)])\n",
    "        return np.array([l[np.argmin(l)] for l in signatures])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mh = MinHashing(100)\n",
    "signatures = [mh.transform(s) for s in sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare minHash signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_signatures(a, b):\n",
    "    \"\"\"Computes the similarity between 2 signatures.\n",
    "        a: numpy array\n",
    "        b: numpy array\n",
    "    \"\"\"\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Signatures lengths differ.\")\n",
    "    return np.mean(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('B2.txt', 'B1.txt'), 1.0), (('A1small.txt', 'A1.txt'), 0.93999999999999995)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimations = {(onlyfiles[i],onlyfiles[j]):compare_signatures(signatures[i], signatures[j]) \n",
    "                for i in range(0, len(signatures)) \n",
    "                for j in range(i+1, len(signatures))}\n",
    "# Show similarity estimations greater than a threshold\n",
    "sorted([(k,v) for k,v in estimations.items() if v > 0.4], key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare minHash estimations against real similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('A1.txt', 'a0200021.txt'), 0.08523326572008115),\n",
       " (('B1.txt', 'a0200014.txt'), 0.084402895054282279),\n",
       " (('B2.txt', 'a0200014.txt'), 0.084402895054282279),\n",
       " (('a0200045.txt', 'a0200021.txt'), 0.082571428571428587),\n",
       " (('B1small.txt', 'B1.txt'), 0.081793353076669961),\n",
       " (('B1small.txt', 'B2.txt'), 0.081793353076669961),\n",
       " (('A1small.txt', 'a0200021.txt'), 0.081456310679611649),\n",
       " (('a0200047.txt', 'a0200014.txt'), 0.079720670391061454),\n",
       " (('B2.txt', 'a0200021.txt'), 0.066504065040650415),\n",
       " (('B1.txt', 'a0200021.txt'), 0.066504065040650415),\n",
       " (('A1small.txt', 'a0200047.txt'), 0.057794994040524433),\n",
       " (('A1.txt', 'a0200014.txt'), 0.056637841577308903),\n",
       " (('A1small.txt', 'a0200014.txt'), 0.053564635435012331),\n",
       " (('a0200060.txt', 'a0200021.txt'), 0.053046184081231573),\n",
       " (('a0200029.txt', 'a0200047.txt'), 0.053036437246963566),\n",
       " (('A1.txt', 'a0200047.txt'), 0.053032015065913374),\n",
       " (('a0200021.txt', 'a0200047.txt'), 0.050838677849037978),\n",
       " (('A1.txt', 'a0200063.txt'), 0.050621733645701932),\n",
       " (('a0200021.txt', 'a0200014.txt'), 0.050181247821540612)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = {(onlyfiles[i],onlyfiles[j]):abs(estimations[(onlyfiles[i],onlyfiles[j])] - similarities[(onlyfiles[i],onlyfiles[j])])\n",
    "           for i in range(0, len(docs)) \n",
    "           for j in range(i+1, len(docs))}\n",
    "# Show errors greater than a threshold (e.g. 5%)\n",
    "sorted([(k,v) for k,v in errors.items() if v > 0.05], key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH: Locality-Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:4: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, matrix, t):\n",
    "        self.t = t\n",
    "        self.M = matrix\n",
    "        self.k = self.M.shape[0]\n",
    "        self.r = int(float(-lambertw(-self.k*log(t))/log(t)))\n",
    "        self.b = int(self.k/self.r)\n",
    "\n",
    "    def index(self):\n",
    "        band_size = self.b+1 if self.b*self.r!=self.k else self.b\n",
    "        hashes = [[norm(self.M[i*self.r:(i+1)*self.r, j]) for j in range(self.M.shape[1])] for i in range(0, band_size)]\n",
    "        #print([[self.M[i:(i+1)*self.r, j] for j in range(self.M.shape[1])] for i in range(0, band_size)])\n",
    "        self.hashtables = []\n",
    "        for j,band in enumerate(hashes):\n",
    "            self.hashtables += [{}]\n",
    "            for i,h in enumerate(band):\n",
    "                if h not in self.hashtables[j]:\n",
    "                    self.hashtables[j][h] = []\n",
    "                self.hashtables[j][h] += [i]\n",
    "        #print(self.hashtables)\n",
    "\n",
    "    def get_pairs(self):\n",
    "        if not hasattr(self, 'hashtables'):\n",
    "            raise Error(\"Call index first\")\n",
    "        pairs = []\n",
    "        for table in self.hashtables:\n",
    "            for bucket in table.values():\n",
    "                pairs += combinations(bucket, 2)\n",
    "        print(Counter(pairs))\n",
    "        return [key for key,value in Counter(pairs).items() if value >= self.t*self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "33\n",
      "3\n",
      "Counter({(7, 9): 34, (1, 4): 29, (6, 9): 3, (6, 7): 3, (2, 7): 1, (5, 9): 1, (12, 14): 1, (2, 9): 1, (7, 12): 1, (5, 12): 1, (5, 7): 1, (2, 12): 1, (2, 5): 1, (9, 12): 1})\n",
      "[(1, 4), (7, 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:6: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "lsh = LSH(np.array(signatures).T, 0.4)\n",
    "print(lsh.k)\n",
    "print(lsh.b)\n",
    "print(lsh.r)\n",
    "lsh.index()\n",
    "print(lsh.get_pairs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
